[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Yash Surange",
    "section": "",
    "text": "ExperienceEducationProjects\n\n\n\n\nStellarDNN Lab, Harvard\nResearch Assistant\n\n\nSep 2022 - Present\n\n\n\nI am working on the application of ASTROMER architecture proposed here on spectral data derived from stars, galaxies and quasars. Currently, my aim is to extract spectra embeddings by training ASTROMER in a self supervised manner.\n\n\n\nUniv.AI, Bengaluru, India\nTeaching Assitant\n\n\nMay 2022 - Present\n\n\n\nAs a teaching assistant, I have helped in course content development for the following courses: AI Basics , Data Science Basics , Convolutional Neural Networks , Language models.\nMy role involved the following:\n\nContent development (theory)- Addition and simplification of concepts for more efficient delivery.\nContent development (practice)- Refinement of labs, exercises related to theory sessions.\nDoubt clearance- Answering student doubts on forum (ed), guiding students during lab hours.\n\n\n\n\nAthenahealth, Bengaluru, India\nAssociate Member of Technical Staff\n\n\nAugust 2021 - July 2022\n\n\n\nI worked as part of test automation team in which I contributed in writing test automation scripts for:\n\nForms meant to be used by providers to collect health information from patients.\nWorkflows to keep track of the vaccination history of patients.\nWorkflows to add medications to a patientâ€™s electronic health record (EHR).\nWorkflows to help providers generate medical bills for patients.\nWorkflow to refer patients to other providers for further treatment.\n\nThis work was done using C#.\n\n\n\nAthenahealth, Bengaluru, India\nIntern\n\n\nJanuary 2021 - July 2021\n\n\n\nI contributed to the migration of performance engineering scripts from LoadRunner to JMeter for both: athenaPractice and athenaFlow.\n\n\n\n\n\nRVCE, Bengaluru, India\nBachelor of Technology in Computer Science\n\n\n2017-2021\n\n\n\n\n\n\nVision Transformer Implementation\n\n\n\n\n\nAim: This project aims to implement Vision transformer paper from scratch in Pytorch.\nMotivation: The motivation behind this project was to get a deeper understanding of vision transformer architecture. It was also intended to impart a better grasp on replicating research papers.\nDataset: MNIST.\nImprovements: Additional things to add: W&B integration, aim for better test accuracy, and code optimization.\nNote about implementation: This implementation was solely meant for learning purposes. This is reflected in the comments supporting lines of code.\n\n\n\n\nAstrospec\n\n\nOngoing\n\n\n\nAim: To extract spectra embeddings by training ASTROMER in a self supervised manner.\nDataset: Custom dataset containing stars, galaxies and quasars from SDSS DR17.\n\n\n\n\nTelco customer churn Prediction\n\n\n\n\n\nAbout: This project was aimed at exploring data, doing a comparative analysis of ML models based on their ability to correctly classify churn customers.\nImportance: Businesses want to maximize their revenue. It is generally easier for a business to retain customers than to onboard new ones. To grow their revenue, businesses must target new customers but also make sure that churn is minimized. If they can identify potential churn customers, they can run programs to retain them.\nDataset: In this project, an imbalanced dataset is used. This closely mimics the real world.\nAlgorithms used:\n\nCategorical Naive Bayes\nKNN\nLogistic Regression\nRandom Forest\nDecision Tree\nDecision Tree with Bagging\nAdaboost\nXGBoost.\n\nMetric used: F1-score is used because we are dealing with an imbalanced class classification problem.\n\n\n\n\nFrom scratch implementations\n\n\n\n\n\nAbout: This project contains some of the most popular algorithms implemented in python only. I took inspiration from open source code repositories to code these algorithms.\nImportance: It is always important to code things from scratch to develop a deep understanding of concepts. I plan to continue adding algorithms to this project.\nAlgorithms implemented:\n\nKNN\nLogistic Regression\nLinear Regression\nDecision Tree\nKmeans clustering"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!!",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Twitter\n  \n  \n    \n     Kaggle\n  \n  \n    \n     Email\n  \n\n      \nI currently work as a research assistant at StellarDNN Lab. I have experience working in industrial and teaching environments in the past. In my industrial role at Athenahealth, I primarily worked on test automation. My teaching role was at Univ.AI where I assisted in developing AI courses.\nYou can read more about me and my work here."
  },
  {
    "objectID": "index.html#latest-blogs",
    "href": "index.html#latest-blogs",
    "title": "Welcome!!",
    "section": "Latest Blogs",
    "text": "Latest Blogs\nClick here to check out more blogs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding paper - Parameter-Efficient Transfer Learning for NLP\n\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2023\n\n\nYash Surange\n\n\n\n\n\n\n  \n\n\n\n\nContinous learning- polishing basics with karpathy\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nYash Surange\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nApr 9, 2023\n\n\nDecoding paper - Parameter-Efficient Transfer Learning for NLP\n\n\nPaper explanation\n\n\n\n\nFeb 25, 2023\n\n\nContinous learning- polishing basics with karpathy\n\n\nFundamentals of neural networks\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-02-25-Continous_learning.html",
    "href": "posts/2023-02-25-Continous_learning.html",
    "title": "Continous learning- polishing basics with karpathy",
    "section": "",
    "text": "I am currently following along with karpathy, completing the video series and completing the exercises in order to polish my fundamentals in neural networks. This has helped me gain a deeper understanding of backpropagation, batchnormalization and it helped me better understand the internals of pytorch. My journey can be found on my github repository"
  },
  {
    "objectID": "posts/2023-02-25-nn_with_karpathy/2023-02-25-Continous_learning.html",
    "href": "posts/2023-02-25-nn_with_karpathy/2023-02-25-Continous_learning.html",
    "title": "Continous learning- polishing basics with karpathy",
    "section": "",
    "text": "I will continue to following along with karpathy, completing the video series and completing the exercises in order to polish my fundamentals in neural networks. This has helped me gain a deeper understanding of backpropagation, batchnormalization and it helped me better understand the internals of pytorch. My journey can be found on my github repository"
  },
  {
    "objectID": "posts/2023-04-9- Paper_implementation_1_adapter/adapter.html",
    "href": "posts/2023-04-9- Paper_implementation_1_adapter/adapter.html",
    "title": "Decoding paper - Parameter-Efficient Transfer Learning for NLP",
    "section": "",
    "text": "1 Lets begin ğŸ\n\n1.0.1 Abstract\nAuthors note that fine tuning of pretrained models is an efficient way of transfer learning in NLP. Transfer learning becomes parameter inefficient when the number of downstream tasks increase as for each task, a new model has to be created. To make this parameter efficient, authors propose to use adapter modules for efficient transfer learning.\nFor each new task, adapters add very few trainable parameters and the parameters of the pretrained model remain unchanged. To demonstrate the effectiveness of this approach, the authors transfer BERT to 26 text classification tasks and achieve within 0.4% of the performance after full fine tuning. Only 3.6% parameters were added per task.\n\n1.0.1.1 Points to note:\n\nThere are different methods of transfer learning in NLP. These are as follows:\n\nTraining the entire architecture (Full fine tuning): The pretrained model is trained in entirety. All the trainable parameters are updated during backpropogation.\nTraining some layers and freezing others: Freezing of the initial layers and training of the later layers. In this case, we have to experimentally determine which layers to be frozen.\nFreezing the entire architecture: We freeze all the layers of the architecture and add new layers on top of them. We train only the additional layers. \n\nReference : Transfer Learning for NLP: Fine-Tuning BERT for Text Classification\n\n\n1.0.2 Introduction\nIn this paper, the authors have targeted the online setting. In this setting tasks arrive in a stream. A better way to understand this is by taking an example. The example is that of google translate. The process involves detection of language, translation of language. These tasks have to be performed on the go. Logically, we would want models to require minimum number of extra parameters to adapt to new task. We also want to make sure that information about the tasks is not lost when we train on new tasks.\nProposed adapter modules are added in between layers of pretrained models.\nThe authors introduce the concept behind adapter based fine tuning by first explaining two common methods of transfer learning: feature based transfer learning and fine tuning.\nLetâ€™s look at equations to understand feature based transfer learning, fine tuning and adapter based fine tuning.\n\nFeature based transfer learning : Consider a function Ï†ğ“Œ(x) (a neural network). This method generates a composition of functions where it composed Ï†ğ“Œ with function ğ“§ğ“‹ to produce ğ“§ğ“‹(Ï†ğ“Œ(x)). Only the new parameters ğ“‹ are trained.\nFine tuning: For each new task, original parameters ğ“Œ are updated.\nAdapter based fine tuning: ğœ“ğ“Œ,ğ“‹ is created where ğ“Œ are taken from pretrained model. Initialisation of parameters ğ“‹ is done such that this new function ğœ“ğ“Œ,ğ“‹(x) â‰ˆ Ï†ğ“Œ(x). Only ğ“‹ parameters are changed during training.Authors also note that if |ğ“‹| << |ğ“Œ|, then many tasks will require only |ğ“Œ| parameters. Adapter based fine tuning thus enables model to be extended to many tasks without affecting previous ones."
  },
  {
    "objectID": "posts/2023-04-9- Paper_implementation_1_adapter/adapter.html#lets-begin",
    "href": "posts/2023-04-9- Paper_implementation_1_adapter/adapter.html#lets-begin",
    "title": "Decoding paper - Parameter-Efficient Transfer Learning for NLP",
    "section": "1 Lets begin ğŸ",
    "text": "1 Lets begin ğŸ\n\n1.1 Abstract\nAuthors note that fine tuning of pretrained models is an efficient way of transfer learning in NLP. Transfer learning becomes parameter inefficient when the number of downstream tasks increase as for each task, a new model has to be created. To make this parameter efficient, authors propose to use adapter modules for efficient transfer learning.\nFor each new task, adapters add very few trainable parameters and the parameters of the pretrained model remain unchanged. To demonstrate the effectiveness of this approach, the authors transfer BERT to 26 text classification tasks and achieve within 0.4% of the performance after full fine tuning. Only 3.6% parameters were added per task.\n\n1.1.1 Points to note:\n\nThere are different methods of transfer learning in NLP. These are as follows:\n\nTraining the entire architecture (Full fine tuning): The pretrained model is trained in entirety. All the trainable parameters are updated during backpropogation.\nTraining some layers and freezing others: Freezing of the initial layers and training of the later layers. In this case, we have to experimentally determine which layers to be frozen.\nFreezing the entire architecture: We freeze all the layers of the architecture and add new layers on top of them. We train only the additional layers.\n\n\n\n1.2 Introduction\nIn this paper, the authors have targeted the online setting. In this setting tasks arrive in a stream. A better way to understand this is by taking an example. The example is that of google translate. The process involves detection of language, translation of language. These tasks have to be performed on the go. Logically, we would want models to require minimum number of extra parameters to adapt to new task. We also want to make sure that information about the tasks is not lost when we train on new tasks."
  }
]